{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gopaltiwari04/LLPS-Score-Predicting-Protein-Phase-Separation-and-Biological-Function-from-Physicochemical-Features/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7Gvtgd0lFtC",
    "outputId": "d1347628-89ae-4cf7-95e6-7a1a4be853db"
   },
   "outputs": [],
   "source": [
    "!pip install biopython\n",
    "!pip install scikit-learn\n",
    "!pip install gensim\n",
    "!pip install iupred2a\n",
    "!pip install requests\n",
    "\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn biopython gensim requests tqdm --quiet\n",
    "\n",
    "print(\"Installing NCBI BLAST+...\")\n",
    "!apt-get update -y -qq\n",
    "!apt-get install -y ncbi-blast+ -qq\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "!seg -\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Gensim for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# BioPython for sequence analysis\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\\nAll libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ELo3wnsGmwB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "ybIplchylGoG",
    "outputId": "7bb36f3b-801a-4bd3-f619-6b99655927b8"
   },
   "outputs": [],
   "source": [
    "# ---------- CODE CELL: Upload, load, clean and combine datasets ----------\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# -- 1) Upload (if missing) --\n",
    "required_files = ['llps_plus.csv', 'llps_minus.csv', 'pdb.csv']\n",
    "missing = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print(\"Please upload the missing CSV files:\", missing)\n",
    "    uploaded = files.upload()\n",
    "    # write any uploaded files to disk (Colab already does this, but safe to write)\n",
    "    for fn, content in uploaded.items():\n",
    "        with open(fn, 'wb') as fh:\n",
    "            fh.write(content)\n",
    "        print(f'User uploaded file \"{fn}\" ({len(content)} bytes)')\n",
    "else:\n",
    "    print(\"All required files already present in the environment.\")\n",
    "\n",
    "# -- 2) Load the CSVs into DataFrames --\n",
    "try:\n",
    "    df_plus = pd.read_csv('llps_plus.csv')\n",
    "    df_minus = pd.read_csv('llps_minus.csv')\n",
    "    df_pdb = pd.read_csv('pdb.csv')\n",
    "    print(f\"Loaded files: {len(df_plus)} rows (llps_plus), {len(df_minus)} rows (llps_minus), {len(df_pdb)} rows (pdb)\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to read one or more CSV files. Check filenames and CSV format.\") from e\n",
    "\n",
    "# ---------- helper: sequence cleaning ----------\n",
    "def clean_sequence(sequence):\n",
    "    \"\"\"\n",
    "    Clean a protein sequence string:\n",
    "      - keep only standard amino-acid one-letter codes (A,C,D,...,Y,U,O)\n",
    "      - uppercase everything\n",
    "      - return None if the resulting sequence is shorter than 11 residues\n",
    "    \"\"\"\n",
    "    if not isinstance(sequence, str):\n",
    "        return None\n",
    "    valid_chars = set(\"ACDEFGHIKLMNPQRSTVWYUO\")  # standard 20 + U and O\n",
    "    cleaned = ''.join([c.upper() for c in sequence if c.upper() in valid_chars])\n",
    "    return cleaned if len(cleaned) > 10 else None\n",
    "\n",
    "# ---------- 3) Label and source columns (fixes you had) ----------\n",
    "df_plus = df_plus.copy()\n",
    "df_minus = df_minus.copy()\n",
    "df_pdb = df_pdb.copy()\n",
    "\n",
    "df_plus['Label']  = 1\n",
    "df_plus['Source'] = 'llps_plus'\n",
    "\n",
    "df_minus['Label'] = 0\n",
    "df_minus['Source'] = 'llps_minus'\n",
    "\n",
    "df_pdb['Label']   = 0\n",
    "df_pdb['Source']  = 'pdb'\n",
    "\n",
    "# ---------- 4) Concatenate ----------\n",
    "df_combined = pd.concat([df_plus, df_minus, df_pdb], ignore_index=True)\n",
    "\n",
    "# ---------- 5) Detect sequence column robustly ----------\n",
    "# prefer 'Sequence' (case-sensitive), then case-insensitive matches, then any col containing 'seq'\n",
    "candidate_cols = [c for c in df_combined.columns if c == 'Sequence' or c.lower() == 'sequence' or 'seq' in c.lower()]\n",
    "if len(candidate_cols) == 0:\n",
    "    raise KeyError(\"No sequence-like column found. Rename your sequence column to 'Sequence' or include 'seq' in the column name.\")\n",
    "sequence_column = candidate_cols[0]\n",
    "print(f\"Using column '{sequence_column}' as the sequence column.\")\n",
    "\n",
    "# ---------- 6) Create a cleaned sequence column and filter invalid sequences ----------\n",
    "df_combined['Clean_Sequence'] = df_combined[sequence_column].astype(str).apply(clean_sequence)\n",
    "\n",
    "before = len(df_combined)\n",
    "df_combined.dropna(subset=['Clean_Sequence'], inplace=True)\n",
    "after = len(df_combined)\n",
    "print(f\"Removed {before - after} rows with invalid or too-short sequences after cleaning.\")\n",
    "\n",
    "# ---------- 7) Remove duplicates (on cleaned sequence) ----------\n",
    "before_dup = len(df_combined)\n",
    "df_combined.drop_duplicates(subset=['Clean_Sequence'], inplace=True)\n",
    "after_dup = len(df_combined)\n",
    "print(f\"Removed {before_dup - after_dup} duplicate sequences (based on Clean_Sequence).\")\n",
    "\n",
    "# ---------- 8) Finalize DataFrame for downstream steps ----------\n",
    "# rename Clean_Sequence -> Sequence (so downstream code uses 'Sequence')\n",
    "df_main = df_combined.copy()\n",
    "df_main.rename(columns={'Clean_Sequence': 'Sequence'}, inplace=True)\n",
    "\n",
    "# If you want to drop the original raw sequence column to avoid confusion:\n",
    "if sequence_column != 'Sequence':\n",
    "    # we already renamed Clean_Sequence to Sequence, so keep that and optionally drop original\n",
    "    df_main.drop(columns=[sequence_column], errors='ignore', inplace=True)\n",
    "\n",
    "# ---------- 9) Quick overview ----------\n",
    "print(\"\\nFinal dataset sample:\")\n",
    "display(df_main.head())\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_main['Label'].value_counts())\n",
    "\n",
    "print(\"\\nNegative-class composition by source (Label==0):\")\n",
    "print(df_main[df_main['Label'] == 0]['Source'].value_counts())\n",
    "\n",
    "# df_main is now ready for feature extraction / modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "id": "Vjap75BErjUy",
    "outputId": "f5483b04-ce3e-4db6-b7da-e5e66192f949"
   },
   "outputs": [],
   "source": [
    "# ---------- CODE CELL 4 (fixed) ----------\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_sequence_length(sequence):\n",
    "    \"\"\"Calculates the length of the sequence.\"\"\"\n",
    "    # Handle arrays/lists by converting to string or getting first element\n",
    "    if isinstance(sequence, (list, np.ndarray)):\n",
    "        if len(sequence) > 0:\n",
    "            sequence = sequence[0] if isinstance(sequence, np.ndarray) else sequence[0]\n",
    "        else:\n",
    "            return 0\n",
    "    return len(sequence) if isinstance(sequence, str) else 0\n",
    "\n",
    "def calculate_hydrophobicity(sequence):\n",
    "    if isinstance(sequence, (list, np.ndarray)):\n",
    "        if len(sequence) > 0:\n",
    "            sequence = sequence[0] if isinstance(sequence, np.ndarray) else sequence[0]\n",
    "        else:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return ProteinAnalysis(sequence).gravy()\n",
    "    except Exception:\n",
    "        return np.nan  # handle potential errors for non-standard sequences\n",
    "\n",
    "def calculate_shannon_entropy(sequence):\n",
    "    \"\"\"Calculates the Shannon entropy of the sequence.\"\"\"\n",
    "    # Handle arrays/lists\n",
    "    if isinstance(sequence, (list, np.ndarray)):\n",
    "        if len(sequence) > 0:\n",
    "            sequence = sequence[0] if isinstance(sequence, np.ndarray) else sequence[0]\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # Now check for empty/invalid sequences\n",
    "    if sequence is None or not isinstance(sequence, str) or len(sequence) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(sequence)\n",
    "    total_len = len(sequence)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        p_i = count / total_len\n",
    "        entropy -= p_i * math.log2(p_i)\n",
    "    return entropy\n",
    "\n",
    "# Check for missing sequences\n",
    "print(f\"Total rows: {len(df_main)}\")\n",
    "print(f\"Sample sequence type: {type(df_main['Sequence'].iloc[0])}\")\n",
    "print(f\"Sample sequence value: {df_main['Sequence'].iloc[0]}\")\n",
    "\n",
    "# Apply these functions using .values to iterate over all rows (including NaN)\n",
    "print(\"\\nCalculating sequence lengths...\")\n",
    "df_main['Length'] = [calculate_sequence_length(seq) for seq in tqdm(df_main['Sequence'].values, desc=\"Length\")]\n",
    "\n",
    "print(\"Calculating hydrophobicity...\")\n",
    "df_main['Hydrophobicity'] = [calculate_hydrophobicity(seq) for seq in tqdm(df_main['Sequence'].values, desc=\"Hydrophobicity\")]\n",
    "\n",
    "print(\"Calculating Shannon entropy...\")\n",
    "df_main['ShannonEntropy'] = [calculate_shannon_entropy(seq) for seq in tqdm(df_main['Sequence'].values, desc=\"Shannon Entropy\")]\n",
    "\n",
    "print(\"\\nGlobal features calculated:\")\n",
    "display(df_main[['Sequence', 'Length', 'Hydrophobicity', 'ShannonEntropy']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "5pXzg610qov1",
    "outputId": "636b3cd1-530f-45cb-b002-3d95e4197a87"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# CODE CELL 5: Functions for LCR and IDR fraction calculation\n",
    "#\n",
    "\n",
    "def calculate_lcr_fraction(sequence, seq_id=\"temp_seq\"):\n",
    "    fasta_in = f\"{seq_id}.fasta\"\n",
    "    with open(fasta_in, \"w\") as f:\n",
    "        f.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "    fasta_out = f\"{seq_id}.seg.fasta\"\n",
    "    command = f\"seg {fasta_in} -x > {fasta_out}\"\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True, stderr=subprocess.DEVNULL)\n",
    "        with open(fasta_out, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            masked_sequence = \"\".join(line.strip() for line in lines if not line.startswith('>'))\n",
    "        os.remove(fasta_in)\n",
    "        os.remove(fasta_out)\n",
    "\n",
    "        # Calculate fraction of 'X's\n",
    "        lcr_length = masked_sequence.count('X')\n",
    "        return lcr_length / len(sequence) if len(sequence) > 0 else 0.0\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        # If SEG fails, clean up and return 0\n",
    "        if os.path.exists(fasta_in): os.remove(fasta_in)\n",
    "        if os.path.exists(fasta_out): os.remove(fasta_out)\n",
    "        return 0.0\n",
    "\n",
    "def get_iupred2a_scores(sequence):\n",
    "    url = \"https://iupred2a.elte.hu/\"\n",
    "    payload = {\n",
    "        'seq': sequence,\n",
    "        'iupred_type': 'long',\n",
    "        'anchor_type': 'none'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, data=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        content = response.text\n",
    "        start_marker = 'var result = '\n",
    "        end_marker = ';'\n",
    "        start_index = content.find(start_marker) + len(start_marker)\n",
    "        end_index = content.find(end_marker, start_index)\n",
    "        json_str = content[start_index:end_index].strip()\n",
    "        data = eval(json_str)\n",
    "        scores = data['iupred2']\n",
    "        return scores\n",
    "\n",
    "    except (requests.exceptions.RequestException, IndexError, SyntaxError, KeyError):\n",
    "        # Return None on any failure\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_idr_fraction(sequence):\n",
    "    scores = get_iupred2a_scores(sequence)\n",
    "    if scores is None:\n",
    "        return np.nan\n",
    "    disordered_residues = 0\n",
    "    stretches = []\n",
    "    start_index = -1\n",
    "    for i, score in enumerate(scores):\n",
    "        if score > 0.5:\n",
    "            if start_index == -1:\n",
    "                start_index = i\n",
    "        else:\n",
    "            if start_index != -1:\n",
    "                stretches.append((start_index, i))\n",
    "                start_index = -1\n",
    "    if start_index != -1:\n",
    "        stretches.append((start_index, len(scores)))\n",
    "\n",
    "    # Count residues only in stretches of length >= 20\n",
    "    for start, end in stretches:\n",
    "        if (end - start) >= 20:\n",
    "            disordered_residues += (end - start)\n",
    "\n",
    "    return disordered_residues / len(sequence) if len(sequence) > 0 else 0.0\n",
    "\n",
    "# --- Apply LCR and IDR calculations ---\n",
    "# Note: These calculations can be slow, especially the API calls for IDR.\n",
    "\n",
    "# To speed up, we will process sequences in batches with delays to respect the API server.\n",
    "# We will first calculate LCR for all sequences.\n",
    "print(\"\\nCalculating LCR Fraction...\")\n",
    "# Apply the function and assign the results to a new column\n",
    "df_main['LCR_Fraction'] = [calculate_lcr_fraction(seq, seq_id=f\"seq_{i}\") for i, seq in tqdm(enumerate(df_main['Sequence'].values), total=len(df_main), desc=\"Calculating LCR Fraction\")]\n",
    "\n",
    "\n",
    "# Now, calculate IDR fraction with rate limiting.\n",
    "idr_fractions = []\n",
    "print(\"\\nCalculating IDR Fraction (this may take a while)...\")\n",
    "for i, seq in tqdm(enumerate(df_main['Sequence'].values), total=len(df_main), desc=\"Fetching IUPred2a scores\"):\n",
    "    idr_fractions.append(calculate_idr_fraction(seq))\n",
    "    time.sleep(0.5) # Be respectful to the API server\n",
    "\n",
    "# Assign the calculated IDR fractions to a new column\n",
    "df_main['IDR_Fraction'] = idr_fractions\n",
    "\n",
    "print(\"\\nStructural features calculated:\")\n",
    "print(df_main[['Sequence', 'LCR_Fraction', 'IDR_Fraction']].head())\n",
    "\n",
    "# Handle any potential failed API calls\n",
    "nan_counts = df_main[['LCR_Fraction', 'IDR_Fraction']].isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(f\"\\nWarning: Missing values found in calculated features:\\n{nan_counts[nan_counts > 0]}\")\n",
    "    print(\"Rows with missing values will be handled in subsequent steps (e.g., dropping before training).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4BGZFkC1JqG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsou43BpD/VwDK03762UeM",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
