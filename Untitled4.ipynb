{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gopaltiwari04/LLPS-Score-Predicting-Protein-Phase-Separation-and-Biological-Function-from-Physicochemical-Features/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7Gvtgd0lFtC",
    "outputId": "d1347628-89ae-4cf7-95e6-7a1a4be853db"
   },
   "outputs": [],
   "source": [
    "!pip install biopython\n",
    "!pip install scikit-learn\n",
    "!pip install gensim\n",
    "!pip install iupred2a\n",
    "!pip install requests\n",
    "\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn biopython gensim requests tqdm --quiet\n",
    "\n",
    "print(\"Installing NCBI BLAST+...\")\n",
    "!apt-get update -y -qq\n",
    "!apt-get install -y ncbi-blast+ -qq\n",
    "print(\"Installation complete.\")\n",
    "\n",
    "!seg -\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Gensim for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# BioPython for sequence analysis\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\\nAll libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ELo3wnsGmwB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "id": "ybIplchylGoG",
    "outputId": "7bb36f3b-801a-4bd3-f619-6b99655927b8"
   },
   "outputs": [],
   "source": [
    "# ---------- CODE CELL: Upload, load, clean and combine datasets ----------\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# -- 1) Upload (if missing) --\n",
    "required_files = ['llps_plus.csv', 'llps_minus.csv', 'pdb.csv']\n",
    "missing = [f for f in required_files if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print(\"Please upload the missing CSV files:\", missing)\n",
    "    uploaded = files.upload()\n",
    "    # write any uploaded files to disk (Colab already does this, but safe to write)\n",
    "    for fn, content in uploaded.items():\n",
    "        with open(fn, 'wb') as fh:\n",
    "            fh.write(content)\n",
    "        print(f'User uploaded file \"{fn}\" ({len(content)} bytes)')\n",
    "else:\n",
    "    print(\"All required files already present in the environment.\")\n",
    "\n",
    "# -- 2) Load the CSVs into DataFrames --\n",
    "try:\n",
    "    df_plus = pd.read_csv('llps_plus.csv')\n",
    "    df_minus = pd.read_csv('llps_minus.csv')\n",
    "    df_pdb = pd.read_csv('pdb.csv')\n",
    "    print(f\"Loaded files: {len(df_plus)} rows (llps_plus), {len(df_minus)} rows (llps_minus), {len(df_pdb)} rows (pdb)\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to read one or more CSV files. Check filenames and CSV format.\") from e\n",
    "\n",
    "# ---------- helper: sequence cleaning ----------\n",
    "def clean_sequence(sequence):\n",
    "    \"\"\"\n",
    "    Clean a protein sequence string:\n",
    "      - keep only standard amino-acid one-letter codes (A,C,D,...,Y,U,O)\n",
    "      - uppercase everything\n",
    "      - return None if the resulting sequence is shorter than 11 residues\n",
    "    \"\"\"\n",
    "    if not isinstance(sequence, str):\n",
    "        return None\n",
    "    valid_chars = set(\"ACDEFGHIKLMNPQRSTVWYUO\")  # standard 20 + U and O\n",
    "    cleaned = ''.join([c.upper() for c in sequence if c.upper() in valid_chars])\n",
    "    return cleaned if len(cleaned) > 10 else None\n",
    "\n",
    "# ---------- 3) Label and source columns (fixes you had) ----------\n",
    "df_plus = df_plus.copy()\n",
    "df_minus = df_minus.copy()\n",
    "df_pdb = df_pdb.copy()\n",
    "\n",
    "df_plus['Label']  = 1\n",
    "df_plus['Source'] = 'llps_plus'\n",
    "\n",
    "df_minus['Label'] = 0\n",
    "df_minus['Source'] = 'llps_minus'\n",
    "\n",
    "df_pdb['Label']   = 0\n",
    "df_pdb['Source']  = 'pdb'\n",
    "\n",
    "# ---------- 4) Concatenate ----------\n",
    "df_combined = pd.concat([df_plus, df_minus, df_pdb], ignore_index=True)\n",
    "\n",
    "# ---------- 5) Detect sequence column robustly ----------\n",
    "# prefer 'Sequence' (case-sensitive), then case-insensitive matches, then any col containing 'seq'\n",
    "candidate_cols = [c for c in df_combined.columns if c == 'Sequence' or c.lower() == 'sequence' or 'seq' in c.lower()]\n",
    "if len(candidate_cols) == 0:\n",
    "    raise KeyError(\"No sequence-like column found. Rename your sequence column to 'Sequence' or include 'seq' in the column name.\")\n",
    "sequence_column = candidate_cols[0]\n",
    "print(f\"Using column '{sequence_column}' as the sequence column.\")\n",
    "\n",
    "# ---------- 6) Create a cleaned sequence column and filter invalid sequences ----------\n",
    "df_combined['Clean_Sequence'] = df_combined[sequence_column].astype(str).apply(clean_sequence)\n",
    "\n",
    "before = len(df_combined)\n",
    "df_combined.dropna(subset=['Clean_Sequence'], inplace=True)\n",
    "after = len(df_combined)\n",
    "print(f\"Removed {before - after} rows with invalid or too-short sequences after cleaning.\")\n",
    "\n",
    "# ---------- 7) Remove duplicates (on cleaned sequence) ----------\n",
    "before_dup = len(df_combined)\n",
    "df_combined.drop_duplicates(subset=['Clean_Sequence'], inplace=True)\n",
    "after_dup = len(df_combined)\n",
    "print(f\"Removed {before_dup - after_dup} duplicate sequences (based on Clean_Sequence).\")\n",
    "\n",
    "# ---------- 8) Finalize DataFrame for downstream steps ----------\n",
    "# rename Clean_Sequence -> Sequence (so downstream code uses 'Sequence')\n",
    "df_main = df_combined.copy()\n",
    "df_main.rename(columns={'Clean_Sequence': 'Sequence'}, inplace=True)\n",
    "\n",
    "# If you want to drop the original raw sequence column to avoid confusion:\n",
    "if sequence_column != 'Sequence':\n",
    "    # we already renamed Clean_Sequence to Sequence, so keep that and optionally drop original\n",
    "    df_main.drop(columns=[sequence_column], errors='ignore', inplace=True)\n",
    "\n",
    "# ---------- 9) Quick overview ----------\n",
    "print(\"\\nFinal dataset sample:\")\n",
    "display(df_main.head())\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df_main['Label'].value_counts())\n",
    "\n",
    "print(\"\\nNegative-class composition by source (Label==0):\")\n",
    "print(df_main[df_main['Label'] == 0]['Source'].value_counts())\n",
    "\n",
    "# df_main is now ready for feature extraction / modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670,
     "referenced_widgets": [
      "68d7b87be5a746eb9ae3da6622c4d42f",
      "d14f6cd313f441a7a026c2e366a1e2ed",
      "d180e12616c0435db0b88ed846c6176f",
      "b93a6a735ec7459ebeca17647c7d9d55",
      "5736686b46bd47539dd205d363da77bf",
      "4c0d642a554f4f0190a9db5ed002f070",
      "ff0489a912d34fdca8ae8faf66750d7d",
      "361851d9c6b047078cd0bc5e7f6580ac",
      "5808fe80c91c411082e6544f914d8877",
      "65144783ad3942a5b87b760e643d7a78",
      "8413c83358c8438f920fa1a3016d3679",
      "a6b4e444967d473c8017c46fa5dc61bc",
      "77e983fed36f49f98fc8237b245b573e",
      "312d301650a94611a86905aaca6680b1",
      "8dcee9c77f0043dd89fbb4cd75400d38",
      "2525d1ecfd5b4d56b0ce0de5ecfdf8d7",
      "278cb1366e574beb926e2a3c382f3bca",
      "d4d191f09b0a4127a1f122ff0327d92e",
      "fe4c0a3b1603431eb15bb7afebdbcc58",
      "d89f7e5c69934884aca1e6bfd5c570a2",
      "2474393e5fa347cb9545dac02a15e48d",
      "1187bd5e6e5c4952ae784c141742fdc0",
      "b7b6fad50a654852814b6d30ce2ba24a",
      "6a1aade65949479eb7888c9812aaaea5",
      "10737d5528014733a95bcba021d5aa16",
      "a93d542baa3c4fd99ef5e43b72de8786",
      "4431c85cfb674592b477f44f763c7fd8",
      "2faf0282efd34f2f8b4ad7f69764c49a",
      "66b1570816a746bbb9f4ce499cda5737",
      "6fcd11a43e1e4ec38fc08d4395c68e80",
      "60a5bb7e87a04d528b6817e49caca6b5",
      "15726037eeaf4894a08ba85bd6b45837",
      "2119c8408a4547b48d560bbcf62782fc"
     ]
    },
    "id": "Vjap75BErjUy",
    "outputId": "f5483b04-ce3e-4db6-b7da-e5e66192f949"
   },
   "outputs": [],
   "source": [
    "# ---------- CODE CELL 4 (fixed) ----------\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_sequence_length(sequence):\n",
    "    \"\"\"Calculates the length of the sequence.\"\"\"\n",
    "    # Handle arrays/lists by converting to string or getting first element\n",
    "    if isinstance(sequence, (list, np.ndarray)):\n",
    "        if len(sequence) > 0:\n",
    "            sequence = sequence[0] if isinstance(sequence, np.ndarray) else sequence[0]\n",
    "        else:\n",
    "            return 0\n",
    "    return len(sequence) if isinstance(sequence, str) else 0\n",
    "\n",
    "def calculate_hydrophobicity(sequence):\n",
    "    if isinstance(sequence, (list, np.ndarray)):\n",
    "        if len(sequence) > 0:\n",
    "            sequence = sequence[0] if isinstance(sequence, np.ndarray) else sequence[0]\n",
    "        else:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return ProteinAnalysis(sequence).gravy()\n",
    "    except Exception:\n",
    "        return np.nan  # handle potential errors for non-standard sequences\n",
    "\n",
    "def calculate_shannon_entropy(sequence):\n",
    "    \"\"\"Calculates the Shannon entropy of the sequence.\"\"\"\n",
    "    # Handle arrays/lists\n",
    "    if isinstance(sequence, (list, np.ndarray)):\n",
    "        if len(sequence) > 0:\n",
    "            sequence = sequence[0] if isinstance(sequence, np.ndarray) else sequence[0]\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    # Now check for empty/invalid sequences\n",
    "    if sequence is None or not isinstance(sequence, str) or len(sequence) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(sequence)\n",
    "    total_len = len(sequence)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        p_i = count / total_len\n",
    "        entropy -= p_i * math.log2(p_i)\n",
    "    return entropy\n",
    "\n",
    "# Check for missing sequences\n",
    "print(f\"Total rows: {len(df_main)}\")\n",
    "print(f\"Sample sequence type: {type(df_main['Sequence'].iloc[0])}\")\n",
    "print(f\"Sample sequence value: {df_main['Sequence'].iloc[0]}\")\n",
    "\n",
    "# Apply these functions using .values to iterate over all rows (including NaN)\n",
    "print(\"\\nCalculating sequence lengths...\")\n",
    "df_main['Length'] = [calculate_sequence_length(seq) for seq in tqdm(df_main['Sequence'].values, desc=\"Length\")]\n",
    "\n",
    "print(\"Calculating hydrophobicity...\")\n",
    "df_main['Hydrophobicity'] = [calculate_hydrophobicity(seq) for seq in tqdm(df_main['Sequence'].values, desc=\"Hydrophobicity\")]\n",
    "\n",
    "print(\"Calculating Shannon entropy...\")\n",
    "df_main['ShannonEntropy'] = [calculate_shannon_entropy(seq) for seq in tqdm(df_main['Sequence'].values, desc=\"Shannon Entropy\")]\n",
    "\n",
    "print(\"\\nGlobal features calculated:\")\n",
    "display(df_main[['Sequence', 'Length', 'Hydrophobicity', 'ShannonEntropy']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619,
     "referenced_widgets": [
      "0b1a215f2cb54adeac33849ec934afc5",
      "866c8db3200e4777978f11044c36cd57",
      "6aa162f8504a410c83d49f8338074120",
      "942d5415a2824dab91f456d82f732348",
      "8c1293631e7e4e6a82985db096201d88",
      "601c96cefc104f2aadb78c5408913fc7",
      "e9382649de6a41c497db178e340c3415",
      "1806458b51954a8eb528b64649d15d6b",
      "b42df2121de74472bb296ec9c470f43e",
      "41278c517d8d403e803c4c34d3bb5fc4",
      "b247c5d212514271b84bcf95eb1faaeb",
      "dd63c4affc5d4a9680913a42870b01ec",
      "6d1e5bc1d0bf405c9eaaea09a1804046",
      "9f9c2d070679415b92fd292d4cb57d68",
      "0fb5de484ae349b59b5a7de0fd9e2987",
      "f3adf2e0f5d147ccab574125d12c34d1",
      "3c5e783a507f419788af865f103210e5",
      "9cb6d6d325e8489aa2c39fa5c6e5cf42",
      "103c7b1e11d64dc1b2072b5717624825",
      "0a95021578e94cbe9cc689444acfb193",
      "c8dbce40e7cb477db3742d7cee81f74d",
      "990f47bdb65c4380b8dc9cf55ba57bef"
     ]
    },
    "id": "5pXzg610qov1",
    "outputId": "636b3cd1-530f-45cb-b002-3d95e4197a87"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# CODE CELL 5: Functions for LCR and IDR fraction calculation\n",
    "#\n",
    "\n",
    "def calculate_lcr_fraction(sequence, seq_id=\"temp_seq\"):\n",
    "    fasta_in = f\"{seq_id}.fasta\"\n",
    "    with open(fasta_in, \"w\") as f:\n",
    "        f.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "    fasta_out = f\"{seq_id}.seg.fasta\"\n",
    "    command = f\"seg {fasta_in} -x > {fasta_out}\"\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True, stderr=subprocess.DEVNULL)\n",
    "        with open(fasta_out, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            masked_sequence = \"\".join(line.strip() for line in lines if not line.startswith('>'))\n",
    "        os.remove(fasta_in)\n",
    "        os.remove(fasta_out)\n",
    "\n",
    "        # Calculate fraction of 'X's\n",
    "        lcr_length = masked_sequence.count('X')\n",
    "        return lcr_length / len(sequence) if len(sequence) > 0 else 0.0\n",
    "\n",
    "    except subprocess.CalledProcessError:\n",
    "        # If SEG fails, clean up and return 0\n",
    "        if os.path.exists(fasta_in): os.remove(fasta_in)\n",
    "        if os.path.exists(fasta_out): os.remove(fasta_out)\n",
    "        return 0.0\n",
    "\n",
    "def get_iupred2a_scores(sequence):\n",
    "    url = \"https://iupred2a.elte.hu/\"\n",
    "    payload = {\n",
    "        'seq': sequence,\n",
    "        'iupred_type': 'long',\n",
    "        'anchor_type': 'none'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url, data=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        content = response.text\n",
    "        start_marker = 'var result = '\n",
    "        end_marker = ';'\n",
    "        start_index = content.find(start_marker) + len(start_marker)\n",
    "        end_index = content.find(end_marker, start_index)\n",
    "        json_str = content[start_index:end_index].strip()\n",
    "        data = eval(json_str)\n",
    "        scores = data['iupred2']\n",
    "        return scores\n",
    "\n",
    "    except (requests.exceptions.RequestException, IndexError, SyntaxError, KeyError):\n",
    "        # Return None on any failure\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_idr_fraction(sequence):\n",
    "    scores = get_iupred2a_scores(sequence)\n",
    "    if scores is None:\n",
    "        return np.nan\n",
    "    disordered_residues = 0\n",
    "    stretches = []\n",
    "    start_index = -1\n",
    "    for i, score in enumerate(scores):\n",
    "        if score > 0.5:\n",
    "            if start_index == -1:\n",
    "                start_index = i\n",
    "        else:\n",
    "            if start_index != -1:\n",
    "                stretches.append((start_index, i))\n",
    "                start_index = -1\n",
    "    if start_index != -1:\n",
    "        stretches.append((start_index, len(scores)))\n",
    "\n",
    "    # Count residues only in stretches of length >= 20\n",
    "    for start, end in stretches:\n",
    "        if (end - start) >= 20:\n",
    "            disordered_residues += (end - start)\n",
    "\n",
    "    return disordered_residues / len(sequence) if len(sequence) > 0 else 0.0\n",
    "\n",
    "# --- Apply LCR and IDR calculations ---\n",
    "# Note: These calculations can be slow, especially the API calls for IDR.\n",
    "\n",
    "# To speed up, we will process sequences in batches with delays to respect the API server.\n",
    "# We will first calculate LCR for all sequences.\n",
    "print(\"\\nCalculating LCR Fraction...\")\n",
    "# Apply the function and assign the results to a new column\n",
    "df_main['LCR_Fraction'] = [calculate_lcr_fraction(seq, seq_id=f\"seq_{i}\") for i, seq in tqdm(enumerate(df_main['Sequence'].values), total=len(df_main), desc=\"Calculating LCR Fraction\")]\n",
    "\n",
    "\n",
    "# Now, calculate IDR fraction with rate limiting.\n",
    "idr_fractions = []\n",
    "print(\"\\nCalculating IDR Fraction (this may take a while)...\")\n",
    "for i, seq in tqdm(enumerate(df_main['Sequence'].values), total=len(df_main), desc=\"Fetching IUPred2a scores\"):\n",
    "    idr_fractions.append(calculate_idr_fraction(seq))\n",
    "    time.sleep(0.5) # Be respectful to the API server\n",
    "\n",
    "# Assign the calculated IDR fractions to a new column\n",
    "df_main['IDR_Fraction'] = idr_fractions\n",
    "\n",
    "print(\"\\nStructural features calculated:\")\n",
    "print(df_main[['Sequence', 'LCR_Fraction', 'IDR_Fraction']].head())\n",
    "\n",
    "# Handle any potential failed API calls\n",
    "nan_counts = df_main[['LCR_Fraction', 'IDR_Fraction']].isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(f\"\\nWarning: Missing values found in calculated features:\\n{nan_counts[nan_counts > 0]}\")\n",
    "    print(\"Rows with missing values will be handled in subsequent steps (e.g., dropping before training).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4BGZFkC1JqG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsou43BpD/VwDK03762UeM",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
